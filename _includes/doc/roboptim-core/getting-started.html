<p>Solving a problem is done through several steps:

<ul>
  <li>Define your cost function by deriving one kind of function, depending
   on whether or not you want to provide a Jacobian and/or a Hessian.</li>
  <li>Define your constraints functions in the same manner.</li>
  <li>Build an instance of problem matching your requirements.</li>
  <li>Use one of the solvers to solve your problem.</li>
</ul></p>

<p>The following example defines a cost function F and two constraints
G0 and G1.</p>

<h3 id="problem">Problem definition</h3>

<p>The problem that will be solved in this tutorial is the 71th
problem of Hock-Schittkowski:

$$min_{x \in \mathbb{R}^4} x_0 x_3 (x_0 + x_1 + x_2) + x_2$$

with the following constraints:

$$x_0 x_1 x_2 x_3 \geq 25$$
$$x_0^2 + x_1^2 + x_2^2 + x_3^2 = 40$$
$$1 \leq x_0, x_1, x_2, x_3 \leq 5$$
</p>


<h3 id="cost">Defining the cost function</h3>

<p>The library contains the following hierarchy of functions:

<ul>
  <li><code>roboptim::Function</code></li>
  <li><code>roboptim::DifferentiableFunction</code></li>
  <li><code>roboptim::TwiceDifferentiableFunction</code></li>
  <li><code>roboptim::QuadraticFunction</code></li>
  <li><code>roboptim::LinearFunction</code></li>
</ul></p>

<p>These types correspond to dense vectors and matrices relying on
Eigen. RobOptim also support sparse matrices and you can even extend
the framework to support your own types.</p>

<p>When defining a new function, you have to derive your new function
  from one of these classes. Depending on the class you derive from,
  you will have to implement one or several methods:

<ul>
  <li><code>impl_compute</code> that returns the function's result has
    to be defined for all functions.</li>
  <li><code>impl_gradient</code> which returns the function's gradient
    is to be defined for DifferentiableFunction and its subclasses.</li>
  <li><code>impl_hessian</code> for TwiceDifferentiableFunction functions and its subclasses.</li>
</ul></p>

<p>It is usually recommended to derive from the deepest possible class
  of the hierarchy (deriving from TwiceDifferentiableFunction is better
  than DifferentiableFunction).</p>

<p>Keep in mind that the type of the function represents the amount of
  information the solver will get, not the real nature of a function
  (it is possible to avoid defining a Hessian by deriving from
  DifferentiableFunction, even if your function can be derived twice).</p>

<p>In the following sample, a TwiceDifferentiableFunction will be
  defined.</p>

<pre><code>
#include &lt;boost/shared_ptr.hpp&gt;

#include &lt;roboptim/core/twice-differentiable-function.hh&gt;
#include &lt;roboptim/core/io.hh&gt;
#include &lt;roboptim/core/solver.hh&gt;
#include &lt;roboptim/core/solver-factory.hh&gt;

using namespace roboptim;

struct F : public TwiceDifferentiableFunction
{
  F () : TwiceDifferentiableFunction (4, 1, &quot;x&#8320; * x&#8323; * (x&#8320; + x&#8321; + x&#8322;) + x&#8322;&quot;)
  {
  }

  void
  impl_compute (result_ref result, const_argument_ref x) const
  {
    result[0] = x[0] * x[3] * (x[0] + x[1] + x[2]) + x[2];
  }

  void
  impl_gradient (gradient_ref grad, const_argument_ref x, size_type) const
  {
    grad &lt;&lt; x[0] * x[3] + x[3] * (x[0] + x[1] + x[2]),
            x[0] * x[3],
            x[0] * x[3] + 1,
            x[0] * (x[0] + x[1] + x[2]);
  }

  void
  impl_hessian (hessian_ref h, const_argument_ref x, size_type) const
  {
    h &lt;&lt; 2 * x[3],               x[3], x[3], 2 * x[0] + x[1] + x[2],
         x[3],                   0.,   0.,   x[0],
         x[3],                   0.,   0.,   x[1],
         2 * x[0] + x[1] + x[2], x[0], x[0], 0.;
  }
};
</code></pre>

<h3 id="constraints">Defining the constraints</h3>

<p>A constraint is no different from a cost function and can be
defined in the same way as a cost function.</p>

<p>The following sample defines two constraints which are twice-differentiable
functions.</p>

<pre><code>
struct G0 : public TwiceDifferentiableFunction
{
  G0 () : TwiceDifferentiableFunction (4, 1, &quot;x&#8320; * x&#8321; * x&#8322; * x&#8323;&quot;)
  {
  }

  void
  impl_compute (result_ref result, const_argument_ref x) const
  {
    result[0] = x[0] * x[1] * x[2] * x[3];
  }

  void
  impl_gradient (gradient_ref grad, const_argument_ref x, size_type) const
  {
    grad &lt;&lt; x[1] * x[2] * x[3],
            x[0] * x[2] * x[3],
            x[0] * x[1] * x[3],
            x[0] * x[1] * x[2];
  }

  void
  impl_hessian (hessian_ref h, const_argument_ref x, size_type) const
  {
    h &lt;&lt; 0.,          x[2] * x[3], x[1] * x[3], x[1] * x[2],
         x[2] * x[3], 0.,          x[0] * x[3], x[0] * x[2],
         x[1] * x[3], x[0] * x[3], 0.,          x[0] * x[1],
         x[1] * x[2], x[0] * x[2], x[0] * x[1], 0.;
  }
};

struct G1 : public TwiceDifferentiableFunction
{
  G1 () : TwiceDifferentiableFunction (4, 1, &quot;x&#8320;&sup2; + x&#8321;&sup2; + x&#8322;&sup2; + x&#8323;&sup2;&quot;)
  {
  }

  void
  impl_compute (result_ref result, const_argument_ref x) const
  {
    result[0] = x[0] * x[0] + x[1] * x[1] + x[2] * x[2] + x[3] * x[3];
  }

  void
  impl_gradient (gradient_ref grad, const_argument_ref x, size_type) const
  {
    grad = 2 * x;
  }

  void
  impl_hessian (hessian_ref h, const_argument_ref x, size_type) const
  {
    h &lt;&lt; 2., 0., 0., 0.,
         0., 2., 0., 0.,
         0., 0., 2., 0.,
         0., 0., 0., 2.;
  }
};
</code></pre>

<h3 id="problem">Building the problem and solving it</h3>

<p>The last part of this tutorial covers how to build a problem and solve
  it. The steps are:

  <ul>
    <li>Instanciate your functions (cost functions and constraints).</li>
    <li>Pass them to the problem.</li>
    <li>Optional: set a starting point.</li>
    <li>Instanciate a solver which solves your class of problem.</li>
    <li>Solve the problem by calling minimum.</li>
  </ul>
</p>

<pre><code>
int run_test ()
{
  typedef Solver&lt;EigenMatrixDense&gt; solver_t;

  // Create cost function.
  boost::shared_ptr&lt;F&gt; f (new F ());

  // Create problem.
  solver_t::problem_t pb (f);

  // Set bounds for all optimization parameters.
  // 1. &lt; x_i &lt; 5. (x_i in [1.;5.])
  for (Function::size_type i = 0; i &lt; pb.function ().inputSize (); ++i)
    pb.argumentBounds ()[i] = Function::makeInterval (1., 5.);

  // Set the starting point.
  Function::vector_t start (pb.function ().inputSize ());
  start[0] = 1., start[1] = 5., start[2] = 5., start[3] = 1.;

  // Create constraints.
  boost::shared_ptr&lt;G0&gt; g0 (new G0 ());
  boost::shared_ptr&lt;G1&gt; g1 (new G1 ());

  F::intervals_t bounds;
  solver_t::problem_t::scaling_t scaling;

  // Add constraints
  bounds.push_back(Function::makeLowerInterval (25.));
  scaling.push_back (1.);
  pb.addConstraint
    (boost::static_pointer_cast&lt;TwiceDifferentiableFunction&gt; (g0),
     bounds, scaling);

  bounds.clear ();
  scaling.clear ();

  bounds.push_back(Function::makeInterval (40., 40.));
  scaling.push_back (1.);
  pb.addConstraint
    (boost::static_pointer_cast&lt;TwiceDifferentiableFunction&gt; (g1),
     bounds, scaling);

  // Initialize solver.

  // Here we are relying on a dummy solver.
  // You may change this string to load the solver you wish to use:
  //  - Ipopt: &quot;ipopt&quot;, &quot;ipopt-sparse&quot;, &quot;ipopt-td&quot;
  //  - Eigen: &quot;eigen-levenberg-marquardt&quot;
  //  etc.
  // The plugin is built for a given solver type, so choose it adequately.
  SolverFactory&lt;solver_t&gt; factory (&quot;dummy-td&quot;, pb);
  solver_t&amp; solver = factory ();

  // Compute the minimum and retrieve the result.
  solver_t::result_t res = solver.minimum ();

  // Display solver information.
  std::cout &lt;&lt; solver &lt;&lt; std::endl;

  // Check if the minimization has succeeded.

  // Process the result
  switch (res.which ())
    {
    case solver_t::SOLVER_VALUE:
      {
        // Get the result.
        Result&amp; result = boost::get&lt;Result&gt; (res);

        // Display the result.
        std::cout &lt;&lt; &quot;A solution has been found: &quot; &lt;&lt; std::endl
                  &lt;&lt; result &lt;&lt; std::endl;

        return 0;
      }

    case solver_t::SOLVER_ERROR:
      {
        std::cout &lt;&lt; &quot;A solution should have been found. Failing...&quot;
                  &lt;&lt; std::endl
                  &lt;&lt; boost::get&lt;SolverError&gt; (res).what ()
                  &lt;&lt; std::endl;

        return 2;
      }

    case solver_t::SOLVER_NO_SOLUTION:
      {
        std::cout &lt;&lt; &quot;The problem has not been solved yet.&quot;
                  &lt;&lt; std::endl;

        return 2;
      }
    }

  // Should never happen.
  assert (0);
  return 42;
}
</code></pre>

<p>This is the last piece of code needed to instantiate and resolve an
  optimization problem with this package. Running this piece of code with an
  appropriate solver set, you will get:</p>

<pre><code>
Solver:
  Problem:
    x&#8320; x&#8323; (x&#8320; + x&#8321; + x&#8322;) + x&#8322; (twice differentiable function)
    Argument's bounds: (1, 5), (1, 5), (1, 5), (1, 5)
    Argument's scaling: 1, 1, 1, 1
    Number of constraints: 2
    Constraint 0
        x&#8320; x&#8321; x&#8322; x&#8323; (twice differentiable function)
        Bounds: (25, inf)
        Scaling: 1
        Initial value: [1](25)
    Constraint 1
        x&#8320;&sup2; + x&#8321;&sup2; + x&#8322;&sup2; + x&#8323;&sup2; (twice differentiable function)
        Bounds: (40, 40)
        Scaling: 1
        Initial value: [1](52) (constraint not satisfied)
    Starting point: [4](1,5,5,1)
    Starting value: [1](16)
    Infinity value (for all functions): inf

A solution has been found:
Result:
  Size (input, output): 4, 1
  X: [4](1,4.743,3.82115,1.37941)
  Value: [1](17.014)
  Constraints values: [2](25,40)
  Lambda: [2](-0.552294,0.161469)
</code></pre>

<p>To see more usage examples, consider looking at the test directory of
  the project which contains the project test suite.</p>
